import os
from typing import List, Optional, Tuple

import gradio as gr
import numpy as np
import torch
from PIL import Image
from huggingface_hub import snapshot_download

# Assume these modules are in the same directory or properly installed
from module.pipeline_fastfit import FastFitPipeline
from parse_utils import DWposeDetector, DensePose, SCHP, multi_ref_cloth_agnostic_mask

PERSON_SIZE = (768, 1024)

# --- Translation Dictionary ---
# All user-facing text is stored here for easy management
translations = {
    "zh": {
        "language": "<h4>ğŸŒ è¯­è¨€</h4>",
        "title": "FastFit: åŠ é€Ÿå¤šå‚è€ƒè™šæ‹Ÿè¯•ç©¿",
        "header_html": """
            <div class="main-header">
                <h1 align="center">FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models</h1>
                <p align="center" style="font-size: 18px;">Supported by <a href="https://lavieai.com/">LavieAI</a> and <a href="https://www.loomlyai.com/zh">LoomlyAI</a></p>            
                <div align="center" style="display: flex; justify-content: center; align-items: center; margin: 20px 0;">
                    <a href="https://github.com/Zheng-Chong/FastFit" style="margin: 0 2px; text-decoration: none;"><img src='https://img.shields.io/badge/arXiv-TODO-red?style=flat&logo=arXiv&logoColor=red' alt='arxiv'></a>
                    <a href='https://huggingface.co/zhengchong/FastFit-MR-1024' style="margin: 0 2px;"><img src='https://img.shields.io/badge/Hugging Face-ckpts-orange?style=flat&logo=HuggingFace&logoColor=orange' alt='huggingface'></a>
                    <a href="https://github.com/Zheng-Chong/FastFit" style="margin: 0 2px;"><img src='https://img.shields.io/badge/GitHub-Repo-blue?style=flat&logo=GitHub' alt='GitHub'></a>
                    <a href="https://fastfit.loomlyai.com" style="margin: 0 2px;"><img src='https://img.shields.io/badge/Demo-Gradio-gold?style=flat&logo=Gradio&logoColor=red' alt='Demo'></a>
                    <a href='https://zheng-chong.github.io/FastFit/' style="margin: 0 2px;"><img src='https://img.shields.io/badge/Webpage-Project-silver?style=flat&logo=&logoColor=orange' alt='webpage'></a>
                    <a href="https://github.com/Zheng-Chong/FastFit/blob/main/LICENSE.md" style="margin: 0 2px;"><img src='https://img.shields.io/badge/License-NonCommercial-lightgreen?style=flat&logo=Lisence' alt='License'></a>
                </div>
            </div>
        """,
        "input_settings": "<h3>ğŸ“¤ è¾“å…¥è®¾ç½®</h3>",
        "person_image_header": "<h4>ğŸ‘¤ äººç‰©å›¾åƒ</h4>",
        "person_image_info": '<p class="constraint-text">ğŸ’¡ å»ºè®®ä¸Šä¼  3:4 å®½é«˜æ¯”çš„å…¨èº«äººç‰©å›¾åƒï¼Œå…¶ä»–æ¯”ä¾‹ä¼šè¢«è‡ªåŠ¨ä¸­å¿ƒè£å‰ª</p>',
        "person_image_label": "ä¸Šä¼ å¾…æ¢è£…çš„äººç‰©å›¾åƒ",
        "ref_garment_header": "<h4>ğŸ‘— å‚è€ƒæœè£…</h4>",
        "ref_garment_info": '<p class="constraint-text">âš ï¸ çº¦æŸæ¡ä»¶ï¼šè£™å­ä¸èƒ½ä¸ä¸Šè¡£æˆ–ä¸‹è¡£åŒæ—¶ä¸Šä¼ </p>',
        "upper_body_label": "ä¸Šè¡£",
        "lower_body_label": "ä¸‹è¡£",
        "dress_label": "è£™å­/è¿ä½“è£…",
        "shoes_label": "é‹å­",
        "bag_label": "åŒ…åŒ…",
        "generate_params": "<h3>âš™ï¸ ç”Ÿæˆå‚æ•°</h3>",
        "inference_settings": "<h4>ğŸ® æ¨ç†è®¾ç½®</h4>",
        "ref_size_label": "å‚è€ƒå›¾åƒå°ºå¯¸",
        "ref_size_info": "é€‰æ‹©å‚è€ƒå›¾åƒçš„é«˜åº¦å°ºå¯¸ï¼ˆ512/768/1024ï¼‰ï¼Œå®½åº¦è‡ªåŠ¨è®¡ç®—ä¿æŒ3:4å®½é«˜æ¯”",
        "steps_label": "æ¨ç†æ­¥æ•°",
        "steps_info": "æ›´å¤šæ­¥æ•°é€šå¸¸è´¨é‡æ›´å¥½ä½†é€Ÿåº¦æ›´æ…¢",
        "guidance_label": "å¼•å¯¼å¼ºåº¦",
        "guidance_info": "æ§åˆ¶ç”Ÿæˆç»“æœä¸å‚è€ƒå›¾åƒçš„è´´åˆåº¦",
        "seed_label": "éšæœºç§å­",
        "seed_info": "å›ºå®šç§å­å¯è·å¾—å¯é‡å¤çš„ç»“æœ",
        "square_mask_label": "ä½¿ç”¨æ–¹å½¢æ©ç ",
        "square_mask_info": "ç”Ÿæˆæ›´è§„æ•´çš„æ¢è£…åŒºåŸŸ",
        "pose_guidance_label": "å¯ç”¨å§¿æ€å¼•å¯¼",
        "pose_guidance_info": "ä½¿ç”¨å§¿æ€ä¿¡æ¯æå‡ç”Ÿæˆè´¨é‡",
        "generate_button": "ğŸš€ å¼€å§‹ç”Ÿæˆ",
        "status_text": "",
        "result_header": "<h3>ğŸ“¸ ç”Ÿæˆç»“æœ</h3>",
        "output_image_label": "ç”Ÿæˆçš„æ¢è£…å›¾åƒ",
        "instructions_html": """
            <div class="info-box">
                <h4>ğŸ’¡ ä½¿ç”¨è¯´æ˜</h4>
                <ul>
                    <li><strong>å›¾åƒè¦æ±‚ï¼š</strong> å»ºè®®ä¸Šä¼ æ¸…æ™°ã€æ­£é¢çš„å…¨èº«äººç‰©å›¾åƒï¼Œåˆ†è¾¨ç‡ä¸ä½äº768x1024ï¼Œå®½é«˜æ¯” 3:4 æ•ˆæœæœ€ä½³</li>
                    <li><strong>å›¾åƒè£å‰ªï¼š</strong> é 3:4 æ¯”ä¾‹çš„å›¾åƒä¼šè¢«è‡ªåŠ¨ä¸­å¿ƒè£å‰ªï¼Œå¯èƒ½ä¸¢å¤±éƒ¨åˆ†å†…å®¹</li>
                    <li><strong>æœè£…çº¦æŸï¼š</strong> è£™å­ï¼ˆè¿ä½“è£…ï¼‰ä¸èƒ½ä¸ä¸Šè¡£æˆ–ä¸‹è¡£åŒæ—¶ä¸Šä¼ </li>
                    <li><strong>å‚æ•°è°ƒèŠ‚ï¼š</strong> å‚è€ƒå›¾åƒå°ºå¯¸å½±å“æœè£…ç»†èŠ‚ï¼›æ¨ç†æ­¥æ•°å½±å“è´¨é‡å’Œé€Ÿåº¦ï¼›å¼•å¯¼å¼ºåº¦æ§åˆ¶æ¢è£…æ•ˆæœ</li>
                </ul>
                <h4>âš ï¸ æ³¨æ„äº‹é¡¹</h4>
                <ul>
                    <li>ç”Ÿæˆæ—¶é—´å–å†³äºç¡¬ä»¶é…ç½®å’Œå‚æ•°è®¾ç½®ï¼Œè¯·è€å¿ƒç­‰å¾…</li>
                    <li>ç”Ÿæˆæ•ˆæœå—è¾“å…¥å›¾åƒè´¨é‡å½±å“ï¼Œå»ºè®®ä½¿ç”¨é«˜è´¨é‡å‚è€ƒå›¾åƒ</li>
                </ul>
            </div>
        """,
        "validation_no_person": "âŒ è¯·ä¸Šä¼ äººç‰©å›¾åƒ",
        "validation_dress_conflict": "âŒ è£™å­ï¼ˆè¿ä½“è£…ï¼‰ä¸èƒ½ä¸ä¸Šè¡£æˆ–ä¸‹è¡£åŒæ—¶ä¸Šä¼ ",
        "validation_no_garment": "âŒ è¯·è‡³å°‘ä¸Šä¼ ä¸€ç§å‚è€ƒæœè£…",
        "validation_pass": "âœ… è¾“å…¥éªŒè¯é€šè¿‡",
        "error_model_not_loaded": "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹",
        "error_pose_estimation_failed": "âŒ å§¿æ€ä¼°è®¡å¤±è´¥",
        "error_no_valid_person_image": "âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„äººç‰©å›¾åƒ",
        "error_generation_failed": "âŒ ç”Ÿæˆå¤±è´¥ï¼šæœªè¿”å›æœ‰æ•ˆå›¾åƒ",
        "error_exception": "âŒ ç”Ÿæˆå¤±è´¥: {}",
        "success_generation": "âœ… ç”ŸæˆæˆåŠŸï¼",
    },
    "en": {
        "language": "<h4>ğŸŒ Language</h4>",
        "title": "FastFit: Multi-Reference Virtual Try-On",
        "header_html": """
            <div class="main-header">
                <h1 align="center">FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models</h1>
                <p align="center" style="font-size: 18px;">Supported by <a href="https://lavieai.com/">LavieAI</a> and <a href="https://www.loomlyai.com/en">LoomlyAI</a></p>            
                <div align="center" style="display: flex; justify-content: center; align-items: center; margin: 20px 0;">
                    <a href="https://github.com/Zheng-Chong/FastFit" style="margin: 0 2px; text-decoration: none;"><img src='https://img.shields.io/badge/arXiv-TODO-red?style=flat&logo=arXiv&logoColor=red' alt='arxiv'></a>
                    <a href='https://huggingface.co/zhengchong/FastFit-MR-1024' style="margin: 0 2px;"><img src='https://img.shields.io/badge/Hugging Face-ckpts-orange?style=flat&logo=HuggingFace&logoColor=orange' alt='huggingface'></a>
                    <a href="https://github.com/Zheng-Chong/FastFit" style="margin: 0 2px;"><img src='https://img.shields.io/badge/GitHub-Repo-blue?style=flat&logo=GitHub' alt='GitHub'></a>
                    <a href="https://fastfit.loomlyai.com" style="margin: 0 2px;"><img src='https://img.shields.io/badge/Demo-Gradio-gold?style=flat&logo=Gradio&logoColor=red' alt='Demo'></a>
                    <a href='https://zheng-chong.github.io/FastFit/' style="margin: 0 2px;"><img src='https://img.shields.io/badge/Webpage-Project-silver?style=flat&logo=&logoColor=orange' alt='webpage'></a>
                    <a href="https://github.com/Zheng-Chong/FastFit/blob/main/LICENSE.md" style="margin: 0 2px;"><img src='https://img.shields.io/badge/License-NonCommercial-lightgreen?style=flat&logo=Lisence' alt='License'></a>
                </div>
            </div>
        """,
        "input_settings": "<h3>ğŸ“¤ Input Settings</h3>",
        "person_image_header": "<h4>ğŸ‘¤ Person Image</h4>",
        "person_image_info": '<p class="constraint-text">ğŸ’¡ Suggest uploading a full-body shot with a 3:4 aspect ratio. Others will be center-cropped.</p>',
        "person_image_label": "Upload an image of the person to try on",
        "ref_garment_header": "<h4>ğŸ‘— Reference Garments</h4>",
        "ref_garment_info": '<p class="constraint-text">âš ï¸ Constraint: A dress cannot be uploaded with a top or bottom.</p>',
        "upper_body_label": "Top",
        "lower_body_label": "Bottom",
        "dress_label": "Dress / Overall",
        "shoes_label": "Shoes",
        "bag_label": "Bag",
        "generate_params": "<h3>âš™ï¸ Generation Parameters</h3>",
        "inference_settings": "<h4>ğŸ® Inference Settings</h4>",
        "ref_size_label": "Reference Image Size",
        "ref_size_info": "Select the height for reference images (512/768/1024). Width is auto-calculated to maintain a 3:4 ratio.",
        "steps_label": "Inference Steps",
        "steps_info": "More steps usually lead to better quality but are slower.",
        "guidance_label": "Guidance Scale",
        "guidance_info": "Controls how closely the output follows the reference images.",
        "seed_label": "Random Seed",
        "seed_info": "A fixed seed ensures reproducible results.",
        "square_mask_label": "Use Square Mask",
        "square_mask_info": "Generates a more regular try-on area.",
        "pose_guidance_label": "Enable Pose Guidance",
        "pose_guidance_info": "Uses pose information to improve generation quality.",
        "generate_button": "ğŸš€ Generate",
        "status_text": "",
        "result_header": "<h3>ğŸ“¸ Generation Result</h3>",
        "output_image_label": "Generated Try-on Image",
        "instructions_html": """
            <div class="info-box">
                <h4>ğŸ’¡ Instructions</h4>
                <ul>
                    <li><strong>Image Requirements:</strong> For best results, use clear, frontal, full-body images with a resolution of at least 768x1024 and a 3:4 aspect ratio.</li>
                    <li><strong>Image Cropping:</strong> Images not in a 3:4 ratio will be automatically center-cropped, which might remove parts of the image.</li>
                    <li><strong>Garment Constraints:</strong> A dress (overall) cannot be used with a top or bottom.</li>
                    <li><strong>Parameter Tuning:</strong> Reference image size affects detail; inference steps affect quality and speed; guidance scale controls the try-on effect.</li>
                </ul>
                <h4>âš ï¸ Notes</h4>
                <ul>
                    <li>Generation time depends on hardware and settings. Please be patient.</li>
                    <li>The result quality is affected by the input image quality. Use high-quality reference images.</li>
                </ul>
            </div>
        """,
        "validation_no_person": "âŒ Please upload a person image.",
        "validation_dress_conflict": "âŒ A dress cannot be uploaded with a top or bottom.",
        "validation_no_garment": "âŒ Please upload at least one reference garment.",
        "validation_pass": "âœ… Input validation passed.",
        "error_model_not_loaded": "âŒ Model not loaded. Please load the model first.",
        "error_pose_estimation_failed": "âŒ Pose estimation failed.",
        "error_no_valid_person_image": "âŒ No valid person image detected.",
        "error_generation_failed": "âŒ Generation failed: No valid image was returned.",
        "error_exception": "âŒ Generation failed: {}",
        "success_generation": "âœ… Generation successful!",
    }
}

def center_crop_to_aspect_ratio(img: Image.Image, target_ratio: float) -> Image.Image:
    width, height = img.size
    current_ratio = width / height
    
    if current_ratio > target_ratio:
        new_width = int(height * target_ratio)
        new_height = height
        left = (width - new_width) // 2
        top = 0
    else:
        new_width = width
        new_height = int(width / target_ratio)
        left = 0
        top = (height - new_height) // 2
    
    return img.crop((left, top, left + new_width, top + new_height))


class FastFitDemo:
    def __init__(
        self, 
        base_model_path: str = "Models/FastFit-MR-1024", 
        util_model_path: str = "Models/Human-Toolkit",
        mixed_precision: str = "bf16",
        device: str = None,
    ):
        # If not exists, download the models
        if not os.path.exists(base_model_path):
            os.makedirs(base_model_path, exist_ok=True)
            snapshot_download(
                repo_id="zhengchong/FastFit-MR-1024",
                local_dir=base_model_path,
                local_dir_use_symlinks=False
            )
        if not os.path.exists(util_model_path):
            os.makedirs(util_model_path, exist_ok=True)
            snapshot_download(
                repo_id="zhengchong/Human-Toolkit",
                local_dir=util_model_path,
                local_dir_use_symlinks=False
            )
            
        self.device = device if device is not None else "cuda" if torch.cuda.is_available() else "cpu"
        self.dwpose_detector = DWposeDetector(pretrained_model_name_or_path=os.path.join(util_model_path, "DWPose"), device='cpu')
        self.densepose_detector = DensePose(model_path=os.path.join(util_model_path, "DensePose"), device=self.device)
        self.schp_lip_detector = SCHP(ckpt_path=os.path.join(util_model_path, "SCHP", "schp-lip.pth"), device=self.device)
        self.schp_atr_detector = SCHP(ckpt_path=os.path.join(util_model_path, "SCHP", "schp-atr.pth"), device=self.device)
        self.pipeline = FastFitPipeline(
            base_model_path=base_model_path,
            device=self.device,
            mixed_precision=mixed_precision,
            allow_tf32=True
        )

    def validate_inputs(self, person_img, upper_img, lower_img, dress_img, shoe_img, bag_img) -> Tuple[bool, str]:
        # MODIFIED: Simplified check for person_img as it's now a direct Image, not a dict.
        if person_img is None:
            return False, "validation_no_person"
        
        has_upper = upper_img is not None
        has_lower = lower_img is not None
        has_dress = dress_img is not None
        
        if has_dress and (has_upper or has_lower):
            return False, "validation_dress_conflict"
        
        if not (has_dress or has_upper or has_lower or shoe_img or bag_img):
            return False, "validation_no_garment"
        
        return True, "validation_pass"
    
    def preprocess_person_image(self, person_img: Image.Image):
        if self.dwpose_detector is None or self.densepose_detector is None or self.schp_lip_detector is None or self.schp_atr_detector is None:
            raise RuntimeError("Model not initialized")
            
        person_img = person_img.convert("RGB")
        person_img = center_crop_to_aspect_ratio(person_img, 3/4)
        person_img = person_img.resize(PERSON_SIZE, Image.LANCZOS)
        
        pose_img = self.dwpose_detector(person_img)
        if not isinstance(pose_img, Image.Image):
            raise RuntimeError("Pose estimation failed")
        
        densepose_arr = np.array(self.densepose_detector(person_img))
        lip_arr = np.array(self.schp_lip_detector(person_img))
        atr_arr = np.array(self.schp_atr_detector(person_img))
        
        return pose_img, densepose_arr, lip_arr, atr_arr
    
    def generate_mask(self, densepose_arr: np.ndarray, lip_arr: np.ndarray, atr_arr: np.ndarray, 
                     square_cloth_mask: bool = False) -> Image.Image:
        return multi_ref_cloth_agnostic_mask(
            densepose_arr, lip_arr, atr_arr,
            square_cloth_mask=square_cloth_mask,
            horizon_expand=True
        )
    
    def prepare_reference_images(self, upper_img, lower_img, dress_img, shoe_img, bag_img, ref_height: int) -> Tuple[List[Image.Image], List[str], List[int]]:
        clothing_ref_size = (int(ref_height * 3 / 4), ref_height)
        accessory_ref_size = (384, 512)
        
        ref_images, ref_labels, ref_attention_masks = [], [], []
        
        categories = [
            (upper_img, "upper"), (lower_img, "lower"), (dress_img, "overall"),
            (shoe_img, "shoe"), (bag_img, "bag")
        ]
        
        for img, label in categories:
            target_size = accessory_ref_size if label in ["shoe", "bag"] else clothing_ref_size
            if img is not None:
                img = img.convert("RGB").resize(target_size, Image.LANCZOS)
                ref_images.append(img)
                ref_labels.append(label)
                ref_attention_masks.append(1)
            else:
                ref_images.append(Image.new("RGB", target_size, color=(0, 0, 0)))
                ref_labels.append(label)
                ref_attention_masks.append(0)
        
        return ref_images, ref_labels, ref_attention_masks
    
    def generate_image(
        self, person_img, upper_img, lower_img, dress_img, shoe_img, bag_img,
        ref_height: int, num_inference_steps: int = 50, guidance_scale: float = 2.5,
        use_square_mask: bool = False, seed: int = 42, enable_pose: bool = True
    ) -> Tuple[Optional[Image.Image], str]:
        
        try:
            is_valid, message_key = self.validate_inputs(person_img, upper_img, lower_img, dress_img, shoe_img, bag_img)
            if not is_valid:
                return None, message_key
            
            if self.pipeline is None:
                return None, "error_model_not_loaded"
            
            # MODIFIED: person_img is now a PIL.Image directly, so no dictionary handling is needed.
            if person_img is None:
                return None, "error_no_valid_person_image"

            processed_person_img = person_img.convert("RGB")
            processed_person_img = center_crop_to_aspect_ratio(processed_person_img, 3/4)
            processed_person_img = processed_person_img.resize(PERSON_SIZE, Image.LANCZOS)
            
            # This function does its own internal processing of the person image
            pose_img, densepose_arr, lip_arr, atr_arr = self.preprocess_person_image(person_img)
            
            # MODIFIED: Since gr.Image is used, there is no user-drawn mask.
            # We always generate the mask automatically.
            mask_img = self.generate_mask(densepose_arr, lip_arr, atr_arr, use_square_mask)
            
            ref_images, ref_labels, ref_attention_masks = self.prepare_reference_images(
                upper_img, lower_img, dress_img, shoe_img, bag_img, ref_height
            )
            
            generator = torch.Generator(device=self.device).manual_seed(seed)
            
            with torch.no_grad():
                result = self.pipeline(
                    person=processed_person_img, mask=mask_img, ref_images=ref_images,
                    ref_labels=ref_labels, ref_attention_masks=ref_attention_masks,
                    pose=pose_img if enable_pose else None, num_inference_steps=num_inference_steps,
                    guidance_scale=guidance_scale, generator=generator, return_pil=True
                )
            
            if isinstance(result, list) and len(result) > 0 and isinstance(result[0], Image.Image):
                return result[0], "success_generation"
            
            return None, "error_generation_failed"
            
        except Exception as e:
            print(f"An exception occurred: {e}")
            return None, f"error_exception:{e}"


def create_demo():
    demo_instance = FastFitDemo()
    
    with gr.Blocks(theme="soft", css="""
        .main-header { text-align: center; margin-bottom: 2rem; }
        .upload-section { border: 2px dashed #e0e0e0; border-radius: 10px; padding: 1rem; margin: 1rem 0; }
        .constraint-text { color: #ff6b6b; font-size: 0.9em; font-style: italic; }
        .info-box { background-color: var(--block-background-fill); border: 1px solid var(--border-color-primary); padding: 1rem; border-radius: 8px; margin-top: 2rem; }
        @media (prefers-color-scheme: light) { .info-box { background-color: #f8f9fa; color: #343a40; border-color: #dee2e6; } }
        @media (prefers-color-scheme: dark) { .info-box { background-color: rgba(255, 255, 255, 0.05); color: #e9ecef; border-color: rgba(255, 255, 255, 0.1); } }
    """) as demo:
        
        
        # --- UI Components ---
        # We define all components here so they can be targeted by the update function
        
        # Header
        header_html = gr.HTML(translations["zh"]["header_html"])
        
        with gr.Row():
            with gr.Column(scale=1):
                input_settings_header = gr.HTML(translations["zh"]["input_settings"])
                with gr.Group():
                    person_image_header = gr.HTML(translations["zh"]["person_image_header"])
                    person_image_info = gr.HTML(translations["zh"]["person_image_info"])
                    # MODIFIED: Changed from gr.ImageMask to gr.Image
                    person_image = gr.Image(label=translations["zh"]["person_image_label"], type="pil", sources=["upload"], height=400)
                
                with gr.Group():
                    ref_garment_header = gr.HTML(translations["zh"]["ref_garment_header"])
                    ref_garment_info = gr.HTML(translations["zh"]["ref_garment_info"])
                    with gr.Row():
                        upper_image = gr.Image(label=translations["zh"]["upper_body_label"], type="pil", sources=["upload"], height=200)
                        lower_image = gr.Image(label=translations["zh"]["lower_body_label"], type="pil", sources=["upload"], height=200)
                    dress_image = gr.Image(label=translations["zh"]["dress_label"], type="pil", sources=["upload"], height=200)
                    with gr.Row():
                        shoe_image = gr.Image(label=translations["zh"]["shoes_label"], type="pil", sources=["upload"], height=200)
                        bag_image = gr.Image(label=translations["zh"]["bag_label"], type="pil", sources=["upload"], height=200)
            
            with gr.Column(scale=1):

                generate_params_header = gr.HTML(translations["zh"]["generate_params"])
                with gr.Group():
                    lang_header = gr.HTML(translations["zh"]["language"])
                    lang_state = gr.State("zh")
                    lang_selector = gr.Radio(
                        ["ä¸­æ–‡", "English"],
                        # label="è¯­è¨€ / Language",
                        show_label=False,
                        value="ä¸­æ–‡",
                        interactive=True,
                )
                with gr.Group():
                    inference_settings_header = gr.HTML(translations["zh"]["inference_settings"])
                    ref_size = gr.Slider(minimum=512, maximum=1024, step=256, value=512, label=translations["zh"]["ref_size_label"], info=translations["zh"]["ref_size_info"])
                    num_steps = gr.Slider(minimum=10, maximum=100, value=30, step=1, label=translations["zh"]["steps_label"], info=translations["zh"]["steps_info"])
                    guidance_scale = gr.Slider(minimum=1.0, maximum=10.0, value=2.5, step=0.1, label=translations["zh"]["guidance_label"], info=translations["zh"]["guidance_info"])
                    seed = gr.Slider(minimum=0, maximum=999999, value=42, step=1, label=translations["zh"]["seed_label"], info=translations["zh"]["seed_info"])
                    with gr.Row():
                        use_square_mask = gr.Checkbox(label=translations["zh"]["square_mask_label"], value=False, info=translations["zh"]["square_mask_info"])
                        enable_pose = gr.Checkbox(label=translations["zh"]["pose_guidance_label"], value=True, info=translations["zh"]["pose_guidance_info"])
                
                generate_btn = gr.Button(translations["zh"]["generate_button"], variant="primary", size="lg")
                status_text = gr.HTML()
                
                result_header = gr.HTML(translations["zh"]["result_header"])
                output_image = gr.Image(label=translations["zh"]["output_image_label"], type="pil", height=400)
        
        instructions_html = gr.HTML(translations["zh"]["instructions_html"])
        
        # --- Event Handlers ---

        def update_language(lang_name: str):
            """Updates all UI text components based on the selected language."""
            lang_code = "en" if lang_name == "English" else "zh"
            t = translations[lang_code]
            return {
                lang_state: lang_code,
                header_html: gr.HTML(value=t["header_html"]),
                lang_header: gr.HTML(value=t["language"]),
                input_settings_header: gr.HTML(value=t["input_settings"]),
                person_image_header: gr.HTML(value=t["person_image_header"]),
                person_image_info: gr.HTML(value=t["person_image_info"]),
                # MODIFIED: Now updates a gr.Image component.
                person_image: gr.Image(label=t["person_image_label"]),
                ref_garment_header: gr.HTML(value=t["ref_garment_header"]),
                ref_garment_info: gr.HTML(value=t["ref_garment_info"]),
                upper_image: gr.Image(label=t["upper_body_label"]),
                lower_image: gr.Image(label=t["lower_body_label"]),
                dress_image: gr.Image(label=t["dress_label"]),
                shoe_image: gr.Image(label=t["shoes_label"]),
                bag_image: gr.Image(label=t["bag_label"]),
                generate_params_header: gr.HTML(value=t["generate_params"]),
                inference_settings_header: gr.HTML(value=t["inference_settings"]),
                ref_size: gr.Slider(label=t["ref_size_label"], info=t["ref_size_info"]),
                num_steps: gr.Slider(label=t["steps_label"], info=t["steps_info"]),
                guidance_scale: gr.Slider(label=t["guidance_label"], info=t["guidance_info"]),
                seed: gr.Slider(label=t["seed_label"], info=t["seed_info"]),
                use_square_mask: gr.Checkbox(label=t["square_mask_label"], info=t["square_mask_info"]),
                enable_pose: gr.Checkbox(label=t["pose_guidance_label"], info=t["pose_guidance_info"]),
                generate_btn: gr.Button(value=t["generate_button"]),
                result_header: gr.HTML(value=t["result_header"]),
                output_image: gr.Image(label=t["output_image_label"]),
                instructions_html: gr.HTML(value=t["instructions_html"]),
            }

        # List of all components that need to be updated
        ui_components = [
            lang_state, header_html, lang_header, input_settings_header, person_image_header, person_image_info, person_image,
            ref_garment_header, ref_garment_info, upper_image, lower_image, dress_image, shoe_image, bag_image,
            generate_params_header, inference_settings_header, ref_size, num_steps, guidance_scale, seed,
            use_square_mask, enable_pose, generate_btn, result_header, output_image, instructions_html
        ]
        
        lang_selector.change(
            fn=update_language,
            inputs=[lang_selector],
            outputs=ui_components
        )
        
        def generation_wrapper(lang_code, *args):
            """A wrapper to handle translation of status messages."""
            img, msg_key = demo_instance.generate_image(*args)
            
            t = translations[lang_code]
            
            # Handle exception messages with placeholders
            if "error_exception:" in msg_key:
                key, error_msg = msg_key.split(":", 1)
                status_message = t[key].format(error_msg)
            else:
                status_message = t.get(msg_key, "Unknown status")

            # Add color styling to the status message
            if "âœ…" in status_message or "successful" in status_message:
                styled_message = f'<p style="color: #51cf66;">{status_message}</p>'
            else:
                styled_message = f'<p style="color: #ff6b6b;">{status_message}</p>'

            return img, styled_message

        generate_btn.click(
            fn=generation_wrapper,
            inputs=[
                lang_state, person_image, upper_image, lower_image, dress_image, 
                shoe_image, bag_image, ref_size, num_steps, guidance_scale,
                use_square_mask, seed, enable_pose
            ],
            outputs=[output_image, status_text]
        )
    
    return demo

if __name__ == "__main__":
    demo = create_demo()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )